{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Text and Natural Language Analytics, Fall 2020\n",
    "\n",
    "### Assignment 7\n",
    "\n",
    "Submitted by - \n",
    "Harsh Dhanuka, hd2457"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install webhoseio\n",
    "# !pip install simhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webhoseio, os\n",
    "from gensim.models import KeyedVectors\n",
    "import logging\n",
    "from simhash import Simhash, SimhashIndex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "import nltk\n",
    "\n",
    "import gensim, operator\n",
    "from scipy import spatial\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Write a Python program that filters out exactly and/or semantically duplicate articles from your Webhose dataset of news articles:\n",
    "\n",
    "### Load the downloaded pre-trained Google Word2Vec model from your computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2Vec Google News model...\n",
      "Finished loading Word2Vec Google News model...\n"
     ]
    }
   ],
   "source": [
    "model_path = '/Users/harshdhanuka/Desktop/Columbia Class Matter/SEM 3/5430 Applied Text NLP/Assignment 6/'\n",
    "\n",
    "def load_wordvec_model(modelName, modelFile, flagBin):\n",
    "    print('Loading ' + modelName + ' model...')\n",
    "    model = KeyedVectors.load_word2vec_format(model_path + modelFile, binary=flagBin)\n",
    "    print('Finished loading ' + modelName + ' model...')\n",
    "    return model\n",
    "\n",
    "model_w2v       = load_wordvec_model('Word2Vec Google News', 'GoogleNews-vectors-negative300.bin.gz', True)\n",
    "#model_fasttext = load_wordvec_model('FastText', 'fastText_wiki_en.vec', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.vector_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to check if the input words are present in Word2vec model vocabulary\n",
    "def vocab_check(vectors, words):\n",
    "    \n",
    "    output = list()\n",
    "    for word in words:\n",
    "        if word in vectors.vocab:\n",
    "            output.append(word.strip())\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate similarity between two strings using a particular word vector model\n",
    "def calc_similarity(input1, input2, vectors):\n",
    "    s1words = set(vocab_check(vectors, input1.split()))\n",
    "    s2words = set(vocab_check(vectors, input2.split()))\n",
    "    \n",
    "    output = vectors.n_similarity(s1words, s2words)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to remove English stopwords\n",
    "def cleanup(input):\n",
    "    # remove English stopwords\n",
    "    input = input.replace(\"'s\", \" \").replace(\"n’t\", \" not\").replace(\"’ve\", \" have\")\n",
    "    input = re.sub(r'[^a-zA-Z0-9 ]', '', input)\n",
    "    return input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load your previously obtained dataset of Webhose news articles\n",
    "\n",
    "I will be using the `Netflix` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_data = []\n",
    "with open('/Users/harshdhanuka/Desktop/Columbia Class Matter/SEM 3/5430 Applied Text NLP/Assignment 7/webhose_netflix.json', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        netflix_data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25288"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(netflix_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I will slice the data to only consider the first 10,000 rows, to save application run-time. The entire 25,000 rows keeps running for many hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_data = netflix_data[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read original dataset titles only, to a new variable called 'feeds'\n",
    "\n",
    "We will base deduplication on article titles only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 13 Reasons Why: The popular Netflix show's creator teases chance of a hopeful ending\n",
      "1 Judge gives control of 'Tiger King' Joe Exotic's zoo to Carole Baskin\n",
      "2 A TV reboot of Bong Joon-ho's acclaimed film Snowpiercer has landed on Netflix — what's the deal?\n",
      "3 2-Pack: Ideaworks Mosquito Killer Lamps (battery powered) 2 for $15\n",
      "4 Already-Obese Average Americans Have Drunk & Eaten Their Way To An Extra 5lbs During Lockdown\n",
      "5 Netflix, Disney join other big brands in support of George Floyd protests on 'Blackout Tuesday'\n",
      "6 Novel Entertainment's First Animated Feature-Length Horrid Henry Special to Air on Netflix – aNb Media\n",
      "7 Anime Based On Best-Selling 1973 Disaster Novel, Japan Sinks: 2020 Lands On Netflix This July\n",
      "8 Tiger King star Carole Baskin’s dead husband’s signature on his will was forged, says sheriff\n",
      "9 All about Netflix’s sci-fi television shows we love\n",
      "10 File:Federation starbase, 2230s.png\n",
      "11 news: Reemerged documentary reveals reason behind Her Majesty’s iconic wave\n",
      "12 Samsung TV Plus Launches LGBTQ+ Streaming Network Revry\n",
      "13 US Says Will Investigate India, Other Nations Over Taxes on Tech Giants | Coastaldigest.com - The Trusted News Portal of India, Coastal Karnataka\n",
      "14 COMPACT PRO G3 | GAMING NATIVE HD SMART ANDROID MINI BLUETOOTH PROJECTOR\n",
      "15 Even with Masterchef, programming more desert than dessert\n",
      "16 Who Is Abigail Koppel? Everything To Know About Billionaire Les Wexner's Wife Who's Charity Received Millions From Jeffrey Epstein Scandal\n",
      "17 Egypt- Legendary director Youssef Chahine's films to\n",
      "18 “Hollywood” Gives 1940s Movie History an Interesting but Flawed Rewrite\n",
      "19 Data centre stocks surge as world shelters online\n",
      "20 Viz Media Brings ‘Megalobox’ Anime To Netflix\n",
      "21 Now that you have reached the end of Netflix...\n",
      "22 Facebook and Twitter need more Peter Thiel and less Jack Dorsey\n",
      "23 Companies back ‘black lives  matter’ campaign\n",
      "24 Carole Baskin awarded Joe Exotic's former zoo\n",
      "25 LG 50\" Class 4K UHD 2160P Smart TV 50UN6950ZUF 2020 Model for $278\n",
      "26 Walmart refuses to sell film on threat to free speech in America\n",
      "27 The will of 'Tiger King' star Carole Baskin's missing husband Don Lewis was forged, sheriff says\n",
      "28 'Black Lives Matter:' Netflix, HBO, and Amazon Prime Video voice their support | Showbiz\n",
      "29 Fascinating study reveals which country has the best Netflix content\n",
      "30 The 9 best Showtime series you can watch right now\n",
      "31 Steve Martin and Martin Short have rescheduled their 'The Funniest Show In Town...At The Moment' tour dates - 03-Jun-2020 - NZ Entertainment news\n",
      "32 'Tiger King' star loses animal park to nemesis he tried to kill - Kogonuso\n",
      "33 \"SAL VELEZ JR.\" The Baka Boyz Show on The City part of DASH Radio - KISS NWA\n",
      "34 BBC’s own Beeb voice assistant enters beta trials\n",
      "35 UnitedHeath Group, YouTube, Airbnb, Lyft Respond To Racial Strife With Donations 06/03/2020\n",
      "36 Judge Gives Carole Baskin the Tiger King’s Zoo – غزة اليوم\n",
      "37 Countdown to the Captain’s ‘Crash Landing’ | Philstar.com\n",
      "38 Choked is Anurag Kashyap's Most Accessible Film: Shabana Azmi - NDTV Movies\n",
      "39 Steve Martin and Martin Short have rescheduled their 'The Funniest Show In Town...At The Moment' tour dates - 03-Jun-2020 - NZ Entertainment news\n",
      "40 Ownership of Joe Exotic's zoo given to rival Carol Baskin from Tiger King | 1 NEWS | TVNZ\n",
      "41 Steve Martin and Martin Short have rescheduled their 'The Funniest Show In Town...At The Moment' tour dates - 03-Jun-2020 - NZ Entertainment news\n",
      "42 George Floyd death: Netflix, WarnerMedia, Ford, other brands take a strong stance\n",
      "43 Steve Martin and Martin Short have rescheduled their 'The Funniest Show In Town...At The Moment' tour dates - 03-Jun-2020 - NZ Entertainment news\n",
      "44 DIVERSITY IN FILMMAKING VIRTUAL PANEL 3 RECAP: DIVERSITY IN ANIMATION\n",
      "45 Out Now: adidas Originals x Angel Chen Ozweego\n",
      "46 Pinpoint Asset Management Ltd Acquires Shares of 12,106 Netflix, Inc. (NASDAQ:NFLX)\n",
      "47 New: The 'Netflix for Jews'\n",
      "48 ‘13 Reasons Why,’ ‘The Twilight Zone,’ ‘El Presidente’: Five series not to be missed in June (VIDEO) | Showbiz\n",
      "49 Netflix, Inc. (NASDAQ:NFLX) Shares Sold by Toronto Dominion Bank\n"
     ]
    }
   ],
   "source": [
    "# Show only the first 50 articles, for reference purpose only.\n",
    "\n",
    "feeds = []\n",
    "i = 0\n",
    "for feed in netflix_data[:50]:\n",
    "    feed['id'] = i\n",
    "    print(feed['id'], str(feed['title']))\n",
    "    i += 1\n",
    "    feeds.append(feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all of the titles, and pass it to the new variable 'feeds'\n",
    "\n",
    "feeds = []\n",
    "i = 0\n",
    "for feed in netflix_data:\n",
    "    feed['id'] = i\n",
    "    #print(feed['id'], str(feed['title']))\n",
    "    i += 1\n",
    "    feeds.append(feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Use LSH (SimHash or MinHash), separately or along with Word2Vec, to deduplicate your Webhose feeds based on titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and print SimHash + Word2Vec similarity based duplicate titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SimHash logger\n",
    "\n",
    "logging.getLogger('simhash').setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SimHash Object, with a random intuitive distance of 25 characters (as guided by professor in class)\n",
    "\n",
    "hamming_distance = 25\n",
    "\n",
    "objs = [(str(feed['id']), Simhash(str(feed['title']))) for feed in feeds]\n",
    "index = SimhashIndex(objs, k = hamming_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define threshold score for the similarity score, for the word 2 vec model, score of  0.7 (as guided by professor in class)\n",
    "\n",
    "threshold_score = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a for loop, to iterate through all the titles, and capture their respective duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_index_list = []   # Initialize the emtpy duplicates indices list\n",
    "count_duplicates = 0  # initialize total duplicates count to 0\n",
    "\n",
    "# Run loop throughout the feeds set, as per he hamming distance of 25, and threshold of 0.7\n",
    "\n",
    "for feed_index in range(len(feeds)):\n",
    "    if int(feed_index) not in dup_index_list:  # make sure the index is not already in the  duplicates list\n",
    "        selected_feed = feeds[feed_index]\n",
    "        feed_hash = Simhash(str(selected_feed['title']))\n",
    "        duplicate_indices = index.get_near_dups(feed_hash) \n",
    "        # the above steps gives us the duplicate indices (id's) for the entire data, including the selected feed index row\n",
    "        \n",
    "        # calculate the similarity score as per word 2 vec model\n",
    "        # I will see that the first entry is kep, and the other duplicates are removed\n",
    "        for dupe in duplicate_indices:\n",
    "            if int(dupe) not in dup_index_list:\n",
    "                try:\n",
    "                    score = calc_similarity(selected_feed['title'], feeds[int(dupe)]['title'], model_w2v)    \n",
    "                except:\n",
    "                    score = 0\n",
    "                \n",
    "                # Remove the duplicate entries which have a similarity score of over 0.7\n",
    "                if score > threshold_score:\n",
    "                    if int(dupe) not in dup_index_list:\n",
    "                        if feeds[int(dupe)]['id'] != selected_feed['id']:    # keep the original first entry item, remove the /duplicates others only\n",
    "                            count_duplicates += 1\n",
    "                            dup_index_list.append(feeds[int(dupe)]['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check duplicate statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The original or raw dataset has a total of: 10000 values or entries or rows\n",
      "\n",
      "The number of duplicates as per the hamming distance of 25, and a similarity score threshold of 0.7 is: 5740\n",
      "\n",
      "Finally, the dataset has: 57.4% duplicates\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print('The original or raw dataset has a total of: ' + str(len(feeds)) + ' values or entries or rows')\n",
    "print()\n",
    "print('The number of duplicates as per the hamming distance of 25, and a similarity score threshold of 0.7 is: ' + str(count_duplicates))\n",
    "print()\n",
    "print('Finally, the dataset has: ' + str((count_duplicates/len(feeds))*100) + '% duplicates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 Carole Baskin awarded Joe Exotic's former zoo\n",
      "27 The will of 'Tiger King' star Carole Baskin's missing husband Don Lewis was forged, sheriff says\n",
      "36 Judge Gives Carole Baskin the Tiger King’s Zoo – غزة اليوم\n",
      "39 Steve Martin and Martin Short have rescheduled their 'The Funniest Show In Town...At The Moment' tour dates - 03-Jun-2020 - NZ Entertainment news\n",
      "41 Steve Martin and Martin Short have rescheduled their 'The Funniest Show In Town...At The Moment' tour dates - 03-Jun-2020 - NZ Entertainment news\n",
      "43 Steve Martin and Martin Short have rescheduled their 'The Funniest Show In Town...At The Moment' tour dates - 03-Jun-2020 - NZ Entertainment news\n",
      "52 Pinpoint Asset Management Ltd Acquires Shares of 12,106 Netflix, Inc. (NASDAQ:NFLX)\n",
      "53 Pinpoint Asset Management Ltd Acquires Shares of 12,106 Netflix, Inc. (NASDAQ:NFLX)\n",
      "54 Fascinating study reveals which country has the best Netflix content\n",
      "57 US To \"Investigate\" India, 9 Other Nations Over Tax On Online Firms\n"
     ]
    }
   ],
   "source": [
    "# Testing if its only pulling out duplicates, or also the original entries\n",
    "\n",
    "for row in sorted(dup_index_list)[:10]:\n",
    "    print(feeds[row]['id'],feeds[row]['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We see that title at index 1 is similar to articles at index 24, 45, 98, 99, etc, but in the duplicates list, we excluded index 1, and put the others, which is exactly what we want. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the final new list of dictionary, without the duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The length of the new list with unique netflix titles is: 4260\n",
      "\n",
      "A random entry (id and title) from the new dataset is: \n",
      "\n",
      "5\n",
      "Netflix, Disney join other big brands in support of George Floyd protests on 'Blackout Tuesday'\n"
     ]
    }
   ],
   "source": [
    "netflix_unique = []\n",
    "\n",
    "for feed in range(len(feeds)):\n",
    "    if int(feed) not in dup_index_list:\n",
    "        netflix_unique.append(feeds[int(feed)])\n",
    "        \n",
    "print()\n",
    "print(\"The length of the new list with unique netflix titles is: \" + str(len(netflix_unique)))\n",
    "print()\n",
    "print(\"A random entry (id and title) from the new dataset is: \")\n",
    "print()\n",
    "print(netflix_unique[5]['id'])\n",
    "print(netflix_unique[5]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The total number of titles or rows in the new unique dataset is: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4260"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print()\n",
    "print(\"The total number of titles or rows in the new unique dataset is: \")\n",
    "print()\n",
    "len(netflix_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Make sure to store entire feeds in a JSON, text or CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unique_data.json', 'w') as f:\n",
    "    json.dump(netflix_unique, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
