{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Text and Natural Language Analytics, Fall 2020\n",
    "\n",
    "### Assignment 5\n",
    "\n",
    "Submitted by - \n",
    "Harsh Dhanuka, hd2457"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from ibm_watson import NaturalLanguageUnderstandingV1\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "from ibm_watson.natural_language_understanding_v1 import Features, EntitiesOptions\n",
    "\n",
    "import spacy\n",
    "from spacy.gold import docs_to_json\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.pipeline import SentenceSegmenter\n",
    "import random\n",
    "\n",
    "import urllib.request as url\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am selecting the 'Netflix' dataset from  Assignment 2 for the analysis.\n",
    "\n",
    "The random news article from this dataset is: http://omgili.com/ri/.wHSUbtEfZSCvFgWhG.N__Y_kk6rEaYdjsrpI1bEeKktgDgldQ2s_6MRmlv4jK7fMNVEhQqU4EXb61rVFualijBYg7orEsJZJyBrzqNk1FQ_2z6DFSGmxcGnRpX.EB__lgRE2mriFDCyaZoJ1tyJAvD8XG37Dct6_XhBQCreko_WuncGLV.lltj.I7NCcFr0  \n",
    "\n",
    "This site now automatically redirects to the link - https://www.stuff.co.nz/entertainment/entertainment-top-stories/300026292/judge-gives-control-of-tiger-king-joe-exotics-zoo-to-carole-baskin \n",
    "\n",
    "I will use this new link for my analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define \n",
    "\n",
    "link = 'https://www.stuff.co.nz/entertainment/entertainment-top-stories/300026292/judge-gives-control-of-tiger-king-joe-exotics-zoo-to-carole-baskin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = url.urlopen(link)\n",
    "raw = html.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Judge gives control of 'Tiger King' Joe Exotic's zoo to Carole Baskin | Stuff.co.nz           news National World Coronavirus Climate Change Politics Business prosper Farming Technology Sport Rugby voices & in depth perspectives Pou Tiaki Spotlight Stuff Nation Cartoons KEA Kids News living Travel Homed Life & Style Entertainment bravo Motoring Food & Wine Oddstuff regions northland Auckland Waikato Bay of Plenty Taranaki hawke's bay manawatu wellington nelson marlborough canterbury south cantebury otago southland more Weather Quizzes Puzzles Newsletters about stuff contribute Advertising Careers Privacy Contact stuff family Play Stuff neighbourly mags4gifts stuff events stuff coupons Entertainment Entertainment Top Stories Judge gives control of 'Tiger King' Joe Exotic's zoo to Carole Baskin 08:27, Jun 03 2020 Facebook Twitter Whats App Reddit Email A federal judge in Oklahoma has awarded ownership of the zoo made famous in Netflix's Tiger King docuseries to\n"
     ]
    }
   ],
   "source": [
    "# Using the code provided by lecturer in Module 3 class excercise to parse text from a web or .html article\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', \n",
    "                               # 'head', 'title', \n",
    "                               'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "text = text_from_html(raw)\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above is the raw text which I will be using for passing to the SVO funstions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Extract and print subject-verb-object (SVO) relations from each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
    "OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubsFromConjunctions(subs):\n",
    "    moreSubs = []\n",
    "    for sub in subs:\n",
    "        # rights is a generator\n",
    "        rights = list(sub.rights)\n",
    "        rightDeps = {tok.lower_ for tok in rights}\n",
    "        if \"and\" in rightDeps:\n",
    "            moreSubs.extend([tok for tok in rights if tok.dep_ in SUBJECTS or tok.pos_ == \"NOUN\"])\n",
    "            if len(moreSubs) > 0:\n",
    "                moreSubs.extend(getSubsFromConjunctions(moreSubs))\n",
    "    return moreSubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getObjsFromConjunctions(objs):\n",
    "    moreObjs = []\n",
    "    for obj in objs:\n",
    "        # rights is a generator\n",
    "        rights = list(obj.rights)\n",
    "        rightDeps = {tok.lower_ for tok in rights}\n",
    "        if \"and\" in rightDeps:\n",
    "            moreObjs.extend([tok for tok in rights if tok.dep_ in OBJECTS or tok.pos_ == \"NOUN\"])\n",
    "            if len(moreObjs) > 0:\n",
    "                moreObjs.extend(getObjsFromConjunctions(moreObjs))\n",
    "    return moreObjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVerbsFromConjunctions(verbs):\n",
    "    moreVerbs = []\n",
    "    for verb in verbs:\n",
    "        rightDeps = {tok.lower_ for tok in verb.rights}\n",
    "        if \"and\" in rightDeps:\n",
    "            moreVerbs.extend([tok for tok in verb.rights if tok.pos_ == \"VERB\"])\n",
    "            if len(moreVerbs) > 0:\n",
    "                moreVerbs.extend(getVerbsFromConjunctions(moreVerbs))\n",
    "    return moreVerbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findSubs(tok):\n",
    "    head = tok.head\n",
    "    while head.pos_ != \"VERB\" and head.pos_ != \"NOUN\" and head.head != head:\n",
    "        head = head.head\n",
    "    if head.pos_ == \"VERB\":\n",
    "        subs = [tok for tok in head.lefts if tok.dep_ == \"SUB\"]\n",
    "        if len(subs) > 0:\n",
    "            verbNegated = isNegated(head)\n",
    "            subs.extend(getSubsFromConjunctions(subs))\n",
    "            return subs, verbNegated\n",
    "        elif head.head != head:\n",
    "            return findSubs(head)\n",
    "    elif head.pos_ == \"NOUN\":\n",
    "        return [head], isNegated(tok)\n",
    "    return [], False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isNegated(tok):\n",
    "    negations = {\"no\", \"not\", \"n't\", \"never\", \"none\"}\n",
    "    for dep in list(tok.lefts) + list(tok.rights):\n",
    "        if dep.lower_ in negations:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findSVs(tokens):\n",
    "    svs = []\n",
    "    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\"]\n",
    "    for v in verbs:\n",
    "        subs, verbNegated = getAllSubs(v)\n",
    "        if len(subs) > 0:\n",
    "            for sub in subs:\n",
    "                svs.append((sub.orth_, \"!\" + v.orth_ if verbNegated else v.orth_))\n",
    "    return svs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getObjsFromPrepositions(deps):\n",
    "    objs = []\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"ADP\" and dep.dep_ == \"prep\":\n",
    "            objs.extend([tok for tok in dep.rights if tok.dep_  in OBJECTS or (tok.pos_ == \"PRON\" and tok.lower_ == \"me\")])\n",
    "    return objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getObjsFromAttrs(deps):\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"NOUN\" and dep.dep_ == \"attr\":\n",
    "            verbs = [tok for tok in dep.rights if tok.pos_ == \"VERB\"]\n",
    "            if len(verbs) > 0:\n",
    "                for v in verbs:\n",
    "                    rights = list(v.rights)\n",
    "                    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "                    objs.extend(getObjsFromPrepositions(rights))\n",
    "                    if len(objs) > 0:\n",
    "                        return v, objs\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getObjFromXComp(deps):\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"VERB\" and dep.dep_ == \"xcomp\":\n",
    "            v = dep\n",
    "            rights = list(v.rights)\n",
    "            objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "            objs.extend(getObjsFromPrepositions(rights))\n",
    "            if len(objs) > 0:\n",
    "                return v, objs\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllSubs(v):\n",
    "    verbNegated = isNegated(v)\n",
    "    subs = [tok for tok in v.lefts if tok.dep_ in SUBJECTS and tok.pos_ != \"DET\"]\n",
    "    if len(subs) > 0:\n",
    "        subs.extend(getSubsFromConjunctions(subs))\n",
    "    else:\n",
    "        foundSubs, verbNegated = findSubs(v)\n",
    "        subs.extend(foundSubs)\n",
    "    return subs, verbNegated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllObjs(v):\n",
    "    # rights is a generator\n",
    "    rights = list(v.rights)\n",
    "    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "    objs.extend(getObjsFromPrepositions(rights))\n",
    "\n",
    "    #potentialNewVerb, potentialNewObjs = getObjsFromAttrs(rights)\n",
    "    #if potentialNewVerb is not None and potentialNewObjs is not None and len(potentialNewObjs) > 0:\n",
    "    #    objs.extend(potentialNewObjs)\n",
    "    #    v = potentialNewVerb\n",
    "\n",
    "    potentialNewVerb, potentialNewObjs = getObjFromXComp(rights)\n",
    "    if potentialNewVerb is not None and potentialNewObjs is not None and len(potentialNewObjs) > 0:\n",
    "        objs.extend(potentialNewObjs)\n",
    "        v = potentialNewVerb\n",
    "    if len(objs) > 0:\n",
    "        objs.extend(getObjsFromConjunctions(objs))\n",
    "    return v, objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findSVOs(tokens):\n",
    "    svos = []\n",
    "    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\" and tok.dep_ != \"aux\"]\n",
    "    for v in verbs:\n",
    "        subs, verbNegated = getAllSubs(v)\n",
    "        # hopefully there are subs, if not, don't examine this verb any longer\n",
    "        if len(subs) > 0:\n",
    "            v, objs = getAllObjs(v)\n",
    "            for sub in subs:\n",
    "                for obj in objs:\n",
    "                    objNegated = isNegated(obj)\n",
    "                    svos.append((sub.lower_, \"!\" + v.lower_ if verbNegated or objNegated else v.lower_, obj.lower_))\n",
    "    return svos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printDeps(toks):\n",
    "    for tok in toks:\n",
    "        print(tok.orth_, tok.dep_, tok.pos_, tok.head.orth_, [t.orth_ for t in tok.lefts], [t.orth_ for t in tok.rights])\n",
    "\n",
    "def testSVOs():\n",
    "    #nlp = English()\n",
    "\n",
    "    tok = nlp(text)\n",
    "    svos = findSVOs(tok)\n",
    "    #printDeps(tok)\n",
    "    print(svos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "The Subject-Verb-Object relations for all the sentences in the article are as follows: \n",
      " \n",
      "[('judge', 'gives', 'control'), ('family', 'play', 'stuff'), ('family', 'play', 'coupons'), ('judge', 'gives', 'control'), ('judge', 'awarded', 'ownership'), ('ownership', 'made', 'famous'), ('palk', 'granted', 'control'), ('passage', 'serving', 'term'), ('term', 'killing', 'tigers'), ('who', 'play', 'him'), ('cage', 'playing', 'exotic'), ('baskin', 'sued', 'passage'), ('attempt', 'paying', 'judgment'), ('attempt', 'paying', 'judgment'), ('baskin', 'take', 'control'), ('attorneys', 'representing', 'corporation'), ('attorneys', '!return', 'messages'), ('messages', 'seeking', 'comment'), ('he', 'repeated', 'plea'), ('woman', 'employs', 'man'), ('man', 'using', 'box'), ('machine', 'outperforms', 'maker'), ('teacher', 'speeding', 'van'), ('driver', 'passes', 'man'), ('couple', 'found', 'blacks'), ('campese', 'warns', 'nz')]\n"
     ]
    }
   ],
   "source": [
    "print(\" \")\n",
    "print(\"The Subject-Verb-Object relations for all the sentences in the article are as follows: \")\n",
    "print(\" \")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    testSVOs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Apply TextRank for ranking and selecting key phrases, print the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRank4Keyword():\n",
    "    \"\"\"Extract keywords from text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.d = 0.85 # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5 # convergence threshold\n",
    "        self.steps = 10 # iteration steps\n",
    "        self.node_weight = None # save keywords and its weight\n",
    "\n",
    "    \n",
    "    def set_stopwords(self, stopwords):  \n",
    "        \"\"\"Set stop words\"\"\"\n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "    \n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        \"\"\"Store those words only in cadidate_pos\"\"\"\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                # Store words only with cadidate POS tag\n",
    "                if token.pos_ in candidate_pos and token.is_stop is False:\n",
    "                    if lower is True:\n",
    "                        selected_words.append(token.text.lower())\n",
    "                    else:\n",
    "                        selected_words.append(token.text)\n",
    "            sentences.append(selected_words)\n",
    "        return sentences\n",
    "        \n",
    "    def get_vocab(self, sentences):\n",
    "        \"\"\"Get all tokens\"\"\"\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "    \n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
    "        token_pairs = list()\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i+1, i+window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "        \n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "    \n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        \"\"\"Get normalized matrix\"\"\"\n",
    "        # Build matrix\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "            \n",
    "        # Get Symmeric matrix\n",
    "        g = self.symmetrize(g)\n",
    "        \n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
    "        \n",
    "        return g_norm\n",
    "\n",
    "    \n",
    "    def get_keywords(self, number=10):\n",
    "        \"\"\"Print top number keywords\"\"\"\n",
    "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        for i, (key, value) in enumerate(node_weight.items()):\n",
    "            print(key + ' - ' + str(value))\n",
    "            if i > number:\n",
    "                break\n",
    "        \n",
    "        \n",
    "    def analyze(self, text, \n",
    "                candidate_pos=['NOUN', 'PROPN'], \n",
    "                window_size=4, lower=False, stopwords=list()):\n",
    "        \"\"\"Main function to analyze text\"\"\"\n",
    "        \n",
    "        # Set stop words\n",
    "        self.set_stopwords(stopwords)\n",
    "        \n",
    "        # Pare text by spaCy\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Filter sentences\n",
    "        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n",
    "        \n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "        \n",
    "        # Get token_pairs from windows\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "        \n",
    "        # Get normalized matrix\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "        \n",
    "        # Initionlization for weight(pagerank value)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "        \n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr)\n",
    "\n",
    "        # Get weight for each node\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "        \n",
    "        self.node_weight = node_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "The Key Phrases according to their ranks for all the sentences in the article are as follows: \n",
      " \n",
      "zoo - 3.0978807385959084\n",
      "Exotic - 2.906616077807699\n",
      "Joe - 2.872306668835595\n",
      "Maldonado - 2.585021255096602\n",
      "Passage - 2.534808113449315\n",
      "Baskin - 2.312616952028586\n",
      "stuff - 1.8689037058692453\n",
      "Australia - 1.8431370298801388\n",
      "Monday - 1.7884209487652747\n",
      "Stuff - 1.767459044284851\n",
      "Entertainment - 1.7580346094613186\n",
      "Tiger - 1.7404878234936123\n",
      "King - 1.732044704858808\n",
      "control - 1.6429326469157965\n",
      "judgment - 1.4917722234350905\n",
      "Privacy - 1.4369425869162553\n",
      "Advertising - 1.4187858520012622\n",
      "Carole - 1.4034608772485013\n",
      "Oklahoma - 1.3632783769123566\n",
      "Careers - 1.35557411287853\n",
      "Judge - 1.3260157032129625\n",
      "Canterbury - 1.2347537890409686\n"
     ]
    }
   ],
   "source": [
    "tr4w = TextRank4Keyword()\n",
    "tr4w.analyze(text, candidate_pos = ['NOUN', 'PROPN',\"ADP\"], window_size=8, lower=False)\n",
    "\n",
    "print(\" \")\n",
    "print(\"The Key Phrases according to their ranks for all the sentences in the article are as follows: \")\n",
    "print(\" \")\n",
    "\n",
    "tr4w.get_keywords(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternate Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate_chunks(text, grammar=r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}'):\n",
    "    import itertools, nltk, string\n",
    "    \n",
    "    # exclude candidates that are stop words or entirely punctuation\n",
    "    punct = set(string.punctuation)\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    # tokenize, POS-tag, and chunk using regular expressions\n",
    "    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
    "    tagged_sents = nltk.pos_tag_sents(nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(text))\n",
    "    all_chunks = list(itertools.chain.from_iterable(nltk.chunk.tree2conlltags(chunker.parse(tagged_sent)) for tagged_sent in tagged_sents))\n",
    "    # join constituent chunk words into a single chunked phrase\n",
    "    candidates = [' '.join(word for word, pos, chunk in group).lower()\n",
    "                  for key, group in itertools.groupby(all_chunks, lambda word__pos__chunk: word__pos__chunk[2] != 'O') if key]\n",
    "\n",
    "    return [cand for cand in candidates\n",
    "            if cand not in stop_words and not all(char in punct for char in cand)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate_words(text, good_tags=set(['JJ','JJR','JJS','NN','NNP','NNS','NNPS'])):\n",
    "    import itertools, nltk, string\n",
    "\n",
    "    # exclude candidates that are stop words or entirely punctuation\n",
    "    punct = set(string.punctuation)\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    # tokenize and POS-tag words\n",
    "    tagged_words = itertools.chain.from_iterable(nltk.pos_tag_sents(nltk.word_tokenize(sent)\n",
    "                                                                    for sent in nltk.sent_tokenize(text)))\n",
    "    # filter on certain POS tags and lowercase all words\n",
    "    candidates = [word.lower() for word, tag in tagged_words\n",
    "                  if tag in good_tags and word.lower() not in stop_words\n",
    "                  and not all(char in punct for char in word)]\n",
    "\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_keyphrases_by_tfidf(texts, candidates='chunks'):\n",
    "    import gensim, nltk\n",
    "    \n",
    "    # extract candidates from each text in texts, either chunks or words\n",
    "    if candidates == 'chunks':\n",
    "        boc_texts = [extract_candidate_chunks(text) for text in texts]\n",
    "    elif candidates == 'words':\n",
    "        boc_texts = [extract_candidate_words(text) for text in texts]\n",
    "    # make gensim dictionary and corpus\n",
    "    dictionary = gensim.corpora.Dictionary(boc_texts)\n",
    "    corpus = [dictionary.doc2bow(boc_text) for boc_text in boc_texts]\n",
    "    # transform corpus with tf*idf model\n",
    "    tfidf = gensim.models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    \n",
    "    return corpus_tfidf, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_keyphrases_by_textrank(text, n_keywords=0.05):\n",
    "    from itertools import takewhile, tee\n",
    "    import operator\n",
    "    import networkx, nltk\n",
    "    \n",
    "    # tokenize for all words, and extract *candidate* words\n",
    "    words = [word.lower()\n",
    "             for sent in nltk.sent_tokenize(text)\n",
    "             for word in nltk.word_tokenize(sent)]\n",
    "    candidates = extract_candidate_words(text)\n",
    "    # build graph, each node is a unique candidate\n",
    "    graph = networkx.Graph()\n",
    "    graph.add_nodes_from(set(candidates))\n",
    "    # iterate over word-pairs, add unweighted edges into graph\n",
    "    def pairwise(iterable):\n",
    "        \"\"\"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\"\"\n",
    "        a, b = tee(iterable)\n",
    "        next(b, None)\n",
    "        return zip(a, b)\n",
    "    for w1, w2 in pairwise(candidates):\n",
    "        if w2:\n",
    "            graph.add_edge(*sorted([w1, w2]))\n",
    "    # score nodes using default pagerank algorithm, sort by score, keep top n_keywords\n",
    "    ranks = networkx.pagerank(graph)\n",
    "    if 0 < n_keywords < 1:\n",
    "        n_keywords = int(round(len(candidates) * n_keywords))\n",
    "    word_ranks = {word_rank[0]: word_rank[1]\n",
    "                  for word_rank in sorted(ranks.items(), key=operator.itemgetter(1), reverse=True)[:n_keywords]}\n",
    "                  #for word_rank in sorted(ranks.iteritems(), key=lambda x: x[1], reverse=True)[:n_keywords]}\n",
    "                  \n",
    "    #sorted(max_value_score.items(), key=operator.itemgetter(1), reverse=True)[:3]\n",
    "    keywords = set(word_ranks.keys())\n",
    "    # merge keywords into keyphrases\n",
    "    keyphrases = {}\n",
    "    j = 0\n",
    "    for i, word in enumerate(words):\n",
    "        if i < j:\n",
    "            continue\n",
    "        if word in keywords:\n",
    "            kp_words = list(takewhile(lambda x: x in keywords, words[i:i+10]))\n",
    "            avg_pagerank = sum(word_ranks[w] for w in kp_words) / float(len(kp_words))\n",
    "            keyphrases[' '.join(kp_words)] = avg_pagerank\n",
    "            # counter as hackish way to ensure merged keyphrases are non-overlapping\n",
    "            j = i + len(kp_words)\n",
    "            \n",
    "    return sorted(keyphrases.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    #return sorted(keyphrases.iteritems(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "The Key Phrases according to their ranks for all the sentences in the article are as follows: \n",
      " \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key Word / Phrase</th>\n",
       "      <th>Rank Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>stuff</td>\n",
       "      <td>0.017673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>maldonado-passage</td>\n",
       "      <td>0.014362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>baskin</td>\n",
       "      <td>0.012792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>zoo</td>\n",
       "      <td>0.012561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>news</td>\n",
       "      <td>0.010272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>joe exotic</td>\n",
       "      <td>0.009716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>monday</td>\n",
       "      <td>0.009134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>judge</td>\n",
       "      <td>0.008648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>0.008489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>entertainment entertainment</td>\n",
       "      <td>0.008489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>netflix</td>\n",
       "      <td>0.008326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>man</td>\n",
       "      <td>0.008099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>control</td>\n",
       "      <td>0.008052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>change</td>\n",
       "      <td>0.008036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>australia</td>\n",
       "      <td>0.008027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Key Word / Phrase  Rank Score\n",
       "0                         stuff    0.017673\n",
       "1             maldonado-passage    0.014362\n",
       "2                        baskin    0.012792\n",
       "3                           zoo    0.012561\n",
       "4                          news    0.010272\n",
       "5                    joe exotic    0.009716\n",
       "6                        monday    0.009134\n",
       "7                         judge    0.008648\n",
       "8                 entertainment    0.008489\n",
       "9   entertainment entertainment    0.008489\n",
       "10                      netflix    0.008326\n",
       "11                          man    0.008099\n",
       "12                      control    0.008052\n",
       "13                       change    0.008036\n",
       "14                    australia    0.008027"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\" \")\n",
    "print(\"The Key Phrases according to their ranks for all the sentences in the article are as follows: \")\n",
    "print(\" \")\n",
    "\n",
    "df = pd.DataFrame(score_keyphrases_by_textrank(text))\n",
    "df.columns = ['Key Word / Phrase', 'Rank Score']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is to note that index 9 has a keyword which is twice of itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Apply LexRank to produce an extractive summary of 5 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSummary(object):\n",
    "\n",
    "    def __init__(self, feeds_str, num_sents):\n",
    "        self.summary = str()\n",
    "        \n",
    "        parser = PlaintextParser.from_string(feeds_str, Tokenizer(\"english\"))\n",
    "        summarizer = LexRankSummarizer()\n",
    "\n",
    "        sentences = summarizer(parser.document, num_sents)  # Summarize the document with 5 sentences\n",
    "        for sentence in sentences:\n",
    "            self.summary += (sentence.__unicode__())\n",
    "\n",
    "    def output(self):\n",
    "        return self.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judge gives control of 'Tiger King' Joe Exotic's zoo to Carole Baskin | Stuff.co.nz           news National World Coronavirus Climate Change Politics Business prosper Farming Technology Sport Rugby voices & in depth perspectives Pou Tiaki Spotlight Stuff Nation Cartoons KEA Kids News living Travel Homed Life & Style Entertainment bravo Motoring Food & Wine Oddstuff regions northland Auckland Waikato Bay of Plenty Taranaki hawke's bay manawatu wellington nelson marlborough canterbury south cantebury otago southland more Weather Quizzes Puzzles Newsletters about stuff contribute Advertising Careers Privacy Contact stuff family Play Stuff neighbourly mags4gifts stuff events stuff coupons Entertainment Entertainment Top Stories Judge gives control of 'Tiger King' Joe Exotic's zoo to Carole Baskin 08:27, Jun 03 2020 Facebook Twitter Whats App Reddit Email A federal judge in Oklahoma has awarded ownership of the zoo made famous in Netflix's Tiger King docuseries to Joe Exotic's chief rival.In a ruling on Monday, US District Judge Scott Palk granted control of the Oklahoma zoo that was previously run by Joseph Maldonado-Passage – also known as Joe Exotic – to Big Cat Rescue Corporation.Maldonado-Passage is currently serving a 22-year federal prison term for killing five tigers and plotting to have Baskin killed.It's Rick Kirkham's turn  Baskin previously sued Maldonado-Passage for trademark and copyright infringements and won a $US1 million ($1.45 million) civil judgment against him.NETFLIX Carole Baskin will take control of Joe Exotic's zoo.\n"
     ]
    }
   ],
   "source": [
    "text_to_sum = TextSummary(text,5)\n",
    "print(text_to_sum.output())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
